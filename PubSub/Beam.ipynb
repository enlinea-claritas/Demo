{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150b7e4-d7ab-44b9-91e3-54f19fe47b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='demo',\n",
    "    package_dir=\n",
    "    packages=find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b556faa8-e7bc-4f24-bee7-dbc734a5c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class CustomOptions(PipelineOptions):\n",
    "    @classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument('--subscription_path')\n",
    "        parser.add_argument('--output_schema')\n",
    "\n",
    "args = PipelineOptions().view_as(CustomOptions)\n",
    "args.subscription_path = 'projects/claritas-bigdata-poc/subscriptions/store'\n",
    "args.output_schema = json.loads(\"\"\"{\"fields\": [\n",
    "    {\"name\": \"order_id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
    "    {\"name\": \"datetime\", \"type\": \"DATETIME\", \"mode\": \"REQUIRED\"},\n",
    "    {\"name\": \"ordered_item\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [\n",
    "        {\"name\": \"item_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
    "        {\"name\": \"item_id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
    "        {\"name\": \"category_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
    "        {\"name\": \"item_price\", \"type\": \"FLOAT\", \"mode\": \"REQUIRED\"},\n",
    "        {\"name\": \"item_qty\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"}\n",
    "    ]}\n",
    "]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267a3693-6861-4d39-beb1-5dba0a594e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast\n",
    "import apache_beam as beam\n",
    "\n",
    "class DecodeByte(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield ast.literal_eval(json.loads(\n",
    "            element.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437f638-c0a8-4afd-8534-30bc7a2f8530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/notebook/lib/python3.8/site-packages/apache_beam/io/gcp/bigquery.py:1683: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  self.table_reference.projectId = pcoll.pipeline.options.view_as(\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/envs/notebook/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpwx98q1d_', 'apache-beam==2.31.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/opt/conda/envs/notebook/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpwx98q1d_', 'apache-beam==2.31.0', '--no-deps', '--only-binary', ':all:', '--python-version', '38', '--implementation', 'cp', '--abi', 'cp38', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.31.0-cp38-cp38-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.8_sdk:2.31.0\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python38-fnapi:2.31.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python38-fnapi:2.31.0\" for Docker environment\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://claritas-bigdata-poc-temp/\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/pickled_main_session...\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/pickled_main_session in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/apache_beam-2.31.0-cp38-cp38-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/apache_beam-2.31.0-cp38-cp38-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://claritas-bigdata-poc-temp/beamapp-meng-0820090436-120825.1629450276.121777/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/meng/.local/share/jupyter/runtime/kernel-70182831-4347-431a-b899-5909f86ceeb1.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/meng/.local/share/jupyter/runtime/kernel-70182831-4347-431a-b899-5909f86ceeb1.json']\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2021-08-20T09:04:40.063024Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2021-08-20_02_04_38-6520034575326755412'\n",
      " location: 'asia-southeast1'\n",
      " name: 'beamapp-meng-0820090436-120825'\n",
      " projectId: 'claritas-bigdata-poc'\n",
      " stageStates: []\n",
      " startTime: '2021-08-20T09:04:40.063024Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2021-08-20_02_04_38-6520034575326755412]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2021-08-20_02_04_38-6520034575326755412\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-southeast1/2021-08-20_02_04_38-6520034575326755412?project=claritas-bigdata-poc\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-08-20_02_04_38-6520034575326755412 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:38.649Z: JOB_MESSAGE_BASIC: Streaming Engine auto-enabled. Use --experiments=disable_streaming_engine to opt out.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:38.650Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:42.576Z: JOB_MESSAGE_WARNING: Autoscaling is enabled for Dataflow Streaming Engine. Workers will scale between 1 and 100 unless maxNumWorkers is specified.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:42.597Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2021-08-20_02_04_38-6520034575326755412. The number of workers will be between 1 and 100.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:42.609Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2021-08-20_02_04_38-6520034575326755412.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:46.848Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-southeast1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.392Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.405Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.444Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.459Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.477Z: JOB_MESSAGE_DETAILED: Expanding SplittableProcessKeyed operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.490Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into streaming Read/Write steps\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.515Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.542Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.562Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.578Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Decode bytestring into [3]: Read from Pub/Sub/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.591Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/AppendDestination into [3]: Decode bytestring\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.604Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/AddInsertIds into [3]: Write to BQ/_StreamToBigQuery/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.620Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/ToHashableTableRef into [3]: Write to BQ/_StreamToBigQuery/AddInsertIds\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.632Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/WithFixedSharding into [3]: Write to BQ/_StreamToBigQuery/ToHashableTableRef\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.646Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/Map(reify_timestamps) into [3]: Write to BQ/_StreamToBigQuery/WithFixedSharding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.659Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/GroupByKey/WriteStream into [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/Map(reify_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.672Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/GroupByKey/MergeBuckets into [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/GroupByKey/ReadStream\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.685Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/FlatMap(restore_timestamps) into [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/GroupByKey/MergeBuckets\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.698Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/DropShard into [3]: Write to BQ/_StreamToBigQuery/CommitInsertIds/FlatMap(restore_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.711Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/FromHashableTableRef into [3]: Write to BQ/_StreamToBigQuery/DropShard\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.724Z: JOB_MESSAGE_DETAILED: Fusing consumer [3]: Write to BQ/_StreamToBigQuery/StreamInsertRows/ParDo(BigQueryWriteFn) into [3]: Write to BQ/_StreamToBigQuery/FromHashableTableRef\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.744Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.757Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.770Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.783Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.819Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.831Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-southeast1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:04:48.845Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2021-08-20_02_04_38-6520034575326755412 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:05:44.198Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 so that the pipeline can catch up with its backlog and keep up with its input rate.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:06:17.025Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2021-08-20T09:06:17.038Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io import ReadFromPubSub, WriteToBigQuery\n",
    "# from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/home/meng/work/.GCP_SA/mlee-claritas-bigdata-poc.json'\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "with beam.Pipeline(options=PipelineOptions(\n",
    "    project='claritas-bigdata-poc',\n",
    "    region='asia-southeast1',\n",
    "    temp_location='gs://claritas-bigdata-poc-temp/',\n",
    "    runner='DataflowRunner',\n",
    "    streaming=True,\n",
    "    save_main_session=True\n",
    ")) as p:\n",
    "    \n",
    "    _ = (p\n",
    "        | 'Read from Pub/Sub' >> ReadFromPubSub(\n",
    "            subscription=args.subscription_path)\n",
    "        | 'Decode bytestring' >> beam.ParDo(DecodeByte())\n",
    "        | 'Write to BQ' >> WriteToBigQuery(\n",
    "            table='test.store',\n",
    "            schema=args.output_schema,\n",
    "            create_disposition='CREATE_IF_NEEDED',\n",
    "            write_disposition='WRITE_APPEND')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42515bcd-02a0-4727-85cb-4852e203283b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
